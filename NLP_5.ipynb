{"cells":[{"cell_type":"markdown","metadata":{"id":"A4WtfJHPIKze"},"source":["**PRACTICAL 5**\n","\n","***Implement a code for aspect mining and topic modeling.***"]},{"cell_type":"markdown","metadata":{"id":"r68c1AR6IQ3M"},"source":["**1. ASPECT MINING**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3249,"status":"ok","timestamp":1728123026724,"user":{"displayName":"Tanmay Dalvi","userId":"05329444289088469822"},"user_tz":-330},"id":"4Mgg00n1IDlO","outputId":"c52ba6ad-04ea-4338-ec07-64dc654c0034"},"outputs":[{"name":"stdout","output_type":"stream","text":["ASPECT EXTRACTION\n","\n","[{'aspect': 'cake', 'description': 'really tasty'}, {'aspect': 'party', 'description': 'amazing'}, {'aspect': 'mom', 'description': 'best'}, {'aspect': 'response', 'description': 'very slow'}, {'aspect': 'trip', 'description': 'very enjoyable'}]\n","\n","SENTIMENT ASSOCIATION\n","\n","[{'aspect': 'cake', 'description': 'really tasty', 'sentiment': Sentiment(polarity=0.2, subjectivity=0.2)}, {'aspect': 'party', 'description': 'amazing', 'sentiment': Sentiment(polarity=0.6000000000000001, subjectivity=0.9)}, {'aspect': 'mom', 'description': 'best', 'sentiment': Sentiment(polarity=1.0, subjectivity=0.3)}, {'aspect': 'response', 'description': 'very slow', 'sentiment': Sentiment(polarity=-0.39000000000000007, subjectivity=0.52)}, {'aspect': 'trip', 'description': 'very enjoyable', 'sentiment': Sentiment(polarity=0.65, subjectivity=0.78)}]\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["[(0,\n","  '0.117*\"amazing\" + 0.117*\"everyone\" + 0.117*\"enjoy\" + 0.117*\"party\" + '\n","  '0.029*\"truffle\" + 0.029*\"much\" + 0.029*\"response\" + 0.029*\"frustrate\" + '\n","  '0.029*\"slow\" + 0.029*\"app\"'),\n"," (1,\n","  '0.082*\"experience\" + 0.082*\"enjoyable\" + 0.082*\"India\" + '\n","  '0.082*\"unforgettable\" + 0.082*\"trip\" + 0.082*\"app\" + 0.082*\"slow\" + '\n","  '0.082*\"response\" + 0.082*\"frustrate\" + 0.020*\"much\"'),\n"," (2,\n","  '0.082*\"chocolate\" + 0.082*\"really\" + 0.082*\"cake\" + 0.082*\"tasty\" + '\n","  '0.082*\"truffle\" + 0.082*\"good\" + 0.082*\"love\" + 0.082*\"mom\" + 0.082*\"much\" '\n","  '+ 0.020*\"frustrate\"')]\n","\n","Document 1 Topic Distribution:\n","[(0, 0.05613074), (1, 0.055941958), (2, 0.8879273)]\n","\n","Document 2 Topic Distribution:\n","[(0, 0.8660328), (1, 0.0669837), (2, 0.06698354)]\n","\n","Document 3 Topic Distribution:\n","[(0, 0.06736673), (1, 0.067134984), (2, 0.86549824)]\n","\n","Document 4 Topic Distribution:\n","[(0, 0.06736115), (1, 0.86550134), (2, 0.06713752)]\n","\n","Document 5 Topic Distribution:\n","[(0, 0.05612808), (1, 0.8879305), (2, 0.055941414)]\n"]}],"source":["import spacy\n","from textblob import TextBlob\n","\n","# Load the spacy model for English\n","sp = spacy.load(\"en_core_web_sm\")\n","\n","# Creating a list of positive and negative sentences.\n","mixed_sen = [\n","    'This chocolate truffle cake is really tasty',\n","    'This party is amazing!',\n","    'My mom is the best!',\n","    'App response is very slow!',\n","    'The trip to India was very enjoyable'\n","]\n","\n","# An empty list for obtaining the extracted aspects from sentences.\n","ext_aspects = []\n","\n","# Performing Aspect Extraction\n","for sen in mixed_sen:\n","    important = sp(sen)\n","    descriptive_item = ''\n","    target = ''\n","\n","    for token in important:\n","        if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n","            target = token.text\n","        if token.pos_ == 'ADJ':\n","            added_terms = ''\n","            for mini_token in token.children:\n","                if mini_token.pos_ != 'ADV':\n","                    continue\n","                added_terms += mini_token.text + ' '\n","            descriptive_item = added_terms + token.text\n","\n","    ext_aspects.append({'aspect': target, 'description': descriptive_item})\n","\n","print(\"ASPECT EXTRACTION\\n\")\n","print(ext_aspects)\n","\n","# Associating Sentiment\n","for aspect in ext_aspects:\n","    aspect['sentiment'] = TextBlob(aspect['description']).sentiment\n","\n","print(\"\\nSENTIMENT ASSOCIATION\\n\")\n","print(ext_aspects)\n","\n","print(\"\")\n","print(\"\")\n","\n","import spacy\n","import gensim\n","import gensim.corpora as corpora\n","from nltk.corpus import stopwords\n","from pprint import pprint\n","import nltk\n","\n","# Download stopwords\n","nltk.download('stopwords')\n","\n","# Load spacy model for lemmatization\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample data for topic modeling\n","documents = [\n","    'This chocolate truffle cake is really tasty',\n","    'The party was amazing and everyone enjoyed it!',\n","    'My mom is the best and she loves me so much',\n","    'The app response is very slow, and it frustrates me',\n","    'The trip to India was very enjoyable and the experience was unforgettable',\n","]\n","\n","# 1. Preprocessing (tokenization, stopwords removal, lemmatization)\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess(doc):\n","    # Tokenize and lemmatize\n","    doc = nlp(doc)\n","    tokens = [token.lemma_ for token in doc if token.is_alpha and token.text.lower() not in stop_words]\n","    return tokens\n","\n","processed_docs = [preprocess(doc) for doc in documents]\n","\n","# 2. Create Dictionary and Corpus\n","# Create a dictionary representation of the documents\n","id2word = corpora.Dictionary(processed_docs)\n","\n","# Create the Bag of Words corpus\n","corpus = [id2word.doc2bow(text) for text in processed_docs]\n","\n","# 3. Applying LDA Model (Topic Modeling)\n","lda_model = gensim.models.LdaMulticore(corpus, id2word=id2word, num_topics=3, passes=10, workers=2, random_state=42)\n","\n","# 4. Output the topics\n","pprint(lda_model.print_topics())\n","\n","# Show the topic distribution for each document\n","for i, topic_distribution in enumerate(lda_model[corpus]):\n","    print(f\"\\nDocument {i + 1} Topic Distribution:\")\n","    print(topic_distribution)\n"]},{"cell_type":"markdown","metadata":{"id":"W3YZh8rKIZNl"},"source":["2. **TOPIC MODELING**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2692,"status":"ok","timestamp":1728122595549,"user":{"displayName":"Tanmay Dalvi","userId":"05329444289088469822"},"user_tz":-330},"id":"amMx9vwnIcx5","outputId":"a6c0439f-856a-4148-cc0a-98b0fd1bea83"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"name":"stdout","output_type":"stream","text":["[(0,\n","  '0.115*\"amazing\" + 0.115*\"everyone\" + 0.115*\"enjoy\" + 0.115*\"party\" + '\n","  '0.031*\"truffle\" + 0.031*\"cake\" + 0.031*\"tasty\" + 0.031*\"really\" + '\n","  '0.031*\"chocolate\" + 0.030*\"much\"'),\n"," (1,\n","  '0.082*\"experience\" + 0.082*\"enjoyable\" + 0.082*\"India\" + '\n","  '0.082*\"unforgettable\" + 0.082*\"trip\" + 0.079*\"app\" + 0.079*\"slow\" + '\n","  '0.079*\"response\" + 0.079*\"frustrate\" + 0.021*\"much\"'),\n"," (2,\n","  '0.080*\"good\" + 0.080*\"love\" + 0.080*\"mom\" + 0.080*\"much\" + '\n","  '0.080*\"chocolate\" + 0.080*\"really\" + 0.080*\"cake\" + 0.080*\"tasty\" + '\n","  '0.079*\"truffle\" + 0.024*\"frustrate\"')]\n","\n","Document 1 Topic Distribution:\n","[(0, 0.056265045), (1, 0.05598232), (2, 0.8877526)]\n","\n","Document 2 Topic Distribution:\n","[(0, 0.86596537), (1, 0.067020714), (2, 0.067013904)]\n","\n","Document 3 Topic Distribution:\n","[(0, 0.06741821), (1, 0.06718809), (2, 0.86539376)]\n","\n","Document 4 Topic Distribution:\n","[(0, 0.067449905), (1, 0.8651176), (2, 0.06743247)]\n","\n","Document 5 Topic Distribution:\n","[(0, 0.056152932), (1, 0.88788563), (2, 0.0559614)]\n"]}],"source":["import spacy\n","import gensim\n","import gensim.corpora as corpora\n","from nltk.corpus import stopwords\n","from pprint import pprint\n","import nltk\n","\n","# Download stopwords\n","nltk.download('stopwords')\n","\n","# Load spacy model for lemmatization\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Sample data for topic modeling\n","documents = [\n","    'This chocolate truffle cake is really tasty',\n","    'The party was amazing and everyone enjoyed it!',\n","    'My mom is the best and she loves me so much',\n","    'The app response is very slow, and it frustrates me',\n","    'The trip to India was very enjoyable and the experience was unforgettable',\n","]\n","\n","# 1. Preprocessing (tokenization, stopwords removal, lemmatization)\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess(doc):\n","    # Tokenize and lemmatize\n","    doc = nlp(doc)\n","    tokens = [token.lemma_ for token in doc if token.is_alpha and token.text.lower() not in stop_words]\n","    return tokens\n","\n","processed_docs = [preprocess(doc) for doc in documents]\n","\n","# 2. Create Dictionary and Corpus\n","# Create a dictionary representation of the documents\n","id2word = corpora.Dictionary(processed_docs)\n","\n","# Create the Bag of Words corpus\n","corpus = [id2word.doc2bow(text) for text in processed_docs]\n","\n","# 3. Applying LDA Model (Topic Modeling)\n","lda_model = gensim.models.LdaMulticore(corpus, id2word=id2word, num_topics=3, passes=10, workers=2, random_state=42)\n","\n","# 4. Output the topics\n","pprint(lda_model.print_topics())\n","\n","# Show the topic distribution for each document\n","for i, topic_distribution in enumerate(lda_model[corpus]):\n","    print(f\"\\nDocument {i + 1} Topic Distribution:\")\n","    print(topic_distribution)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMSPrTmirP0Oflssn3EFgNa","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
