{"cells":[{"cell_type":"markdown","metadata":{"id":"Bs1mr7xx7AHq"},"source":["**PRACTICAL 6**\n","\n","***Practical Python Implementation of Advanced Tokenization Techniques***"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":968,"status":"ok","timestamp":1728371028360,"user":{"displayName":"Tanmay Dalvi","userId":"05329444289088469822"},"user_tz":-330},"id":"5bMM4NPF64NX","outputId":"fb04bbd8-0e46-4e4a-87db-2b5847a5ae07"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["Basic Word Tokenization (NLTK):\n","['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'crucial', 'technology', 'for', 'modern', 'applications', 'like', 'chatbots', ',', 'translation', ',', 'and', 'AI', '.']\n","\n","Subword Tokenization (BERT - WordPiece):\n","['natural', 'language', 'processing', '(', 'nl', '##p', ')', 'is', 'a', 'crucial', 'technology', 'for', 'modern', 'applications', 'like', 'chat', '##bots', ',', 'translation', ',', 'and', 'ai', '.']\n","\n","Byte Pair Encoding (GPT-2):\n","['Natural', 'Ġlanguage', 'Ġprocessing', 'Ġ(', 'N', 'LP', ')', 'Ġis', 'Ġa', 'Ġcrucial', 'Ġtechnology', 'Ġfor', 'Ġmodern', 'Ġapplications', 'Ġlike', 'Ġchat', 'bots', ',', 'Ġtranslation', ',', 'Ġand', 'ĠAI', '.']\n","\n","SentencePiece Tokenization (RoBERTa):\n","['Natural', 'Ġlanguage', 'Ġprocessing', 'Ġ(', 'N', 'LP', ')', 'Ġis', 'Ġa', 'Ġcrucial', 'Ġtechnology', 'Ġfor', 'Ġmodern', 'Ġapplications', 'Ġlike', 'Ġchat', 'bots', ',', 'Ġtranslation', ',', 'Ġand', 'ĠAI', '.']\n","\n","Custom SentencePiece Tokenizer (Training):\n","['▁', 'N', 'at', 'u', 'r', 'a', 'l', '▁', 'l', 'a', 'ng', 'u', 'a', 'g', 'e', '▁', 'p', 'r', 'o', 'c', 'ess', 'i', 'ng', '▁', '(', 'N', 'LP)', '▁', 'i', 's', '▁', 'a', '▁', 'c', 'r', 'u', 'c', 'i', 'a', 'l', '▁', 't', 'e', 'c', 'h', 'n', 'o', 'l', 'o', 'g', 'y', '▁', 'f', 'o', 'r', '▁', 'm', 'o', 'd', 'e', 'r', 'n', '▁', 'a', 'p', 'p', 'l', 'i', 'c', 'a', 'ti', 'o', 'n', 's', '▁', 'l', 'i', 'k', 'e', '▁', 'c', 'h', 'at', 'b', 'o', 't', 's', ',', '▁', 't', 'r', 'a', 'n', 's', 'l', 'a', 'ti', 'o', 'n', ',', '▁', 'a', 'n', 'd', '▁', 'A', 'I', '.']\n"]}],"source":["# Importing necessary libraries\n","from transformers import BertTokenizer, GPT2Tokenizer, RobertaTokenizer\n","import sentencepiece as spm\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","# Sample text\n","text = \"Natural language processing (NLP) is a crucial technology for modern applications like chatbots, translation, and AI.\"\n","\n","# 1. Basic Word Tokenization (NLTK)\n","print(\"Basic Word Tokenization (NLTK):\")\n","word_tokens = word_tokenize(text)\n","print(word_tokens)\n","\n","# 2. Subword Tokenization (BERT's WordPiece Tokenizer)\n","print(\"\\nSubword Tokenization (BERT - WordPiece):\")\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","bert_tokens = bert_tokenizer.tokenize(text)\n","print(bert_tokens)\n","\n","# 3. Byte Pair Encoding (BPE) with GPT-2 Tokenizer\n","print(\"\\nByte Pair Encoding (GPT-2):\")\n","gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","gpt2_tokens = gpt2_tokenizer.tokenize(text)\n","print(gpt2_tokens)\n","\n","# 4. SentencePiece Tokenization (Pretrained on RoBERTa)\n","print(\"\\nSentencePiece Tokenization (RoBERTa):\")\n","roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","roberta_tokens = roberta_tokenizer.tokenize(text)\n","print(roberta_tokens)\n","\n","# 5. Train your own SentencePiece tokenizer (for custom data)\n","print(\"\\nCustom SentencePiece Tokenizer (Training):\")\n","\n","# You would typically train on large text data, but here we simulate with small data\n","sample_data = \"Natural language processing is essential for modern AI applications.\"\n","with open(\"sample_text.txt\", \"w\") as f:\n","    f.write(sample_data)\n","\n","# Train SentencePiece model with smaller vocabulary size\n","spm.SentencePieceTrainer.Train('--input=sample_text.txt --model_prefix=m --vocab_size=28')\n","sp = spm.SentencePieceProcessor(model_file='m.model')\n","\n","# Tokenize using custom SentencePiece model\n","sentencepiece_tokens = sp.encode_as_pieces(text)\n","print(sentencepiece_tokens)\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNdVKxR2VtQYBoWR7pxJSWj","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
